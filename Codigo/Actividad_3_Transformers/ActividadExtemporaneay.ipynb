{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUPO:\n",
    "HENNY ROCIO CARRILLO - hcarril42467@universidadean.edu.co\n",
    "WILBER ALEXANDER RODRIGUEZ CASTRO - wrodrig96546@universidadean.edu.co\n",
    "\n",
    "TEMA: Actividad evaluativa. Clasificación de Noticias Usando RNNs y LSTMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Columna1                                            Enlaces  \\\n",
      "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
      "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "\n",
      "                                              Título  \\\n",
      "0  Operador de grúa quedó inconsciente tras agres...   \n",
      "1   Usaquén, primera en infracciones por mal parqueo   \n",
      "2  'Me atracaron y vi un arma que me heló la sang...   \n",
      "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
      "4  Radicado primer proyecto que autorizaría union...   \n",
      "\n",
      "                                                info  \\\n",
      "0  El conductor de una moto le lanzó el casco y p...   \n",
      "1  La localidad ocupa el primer lugar en comparen...   \n",
      "2  Un ciudadano relata cómo cuatro hombres lo rob...   \n",
      "3  Las zonas de restaurantes se convierten en par...   \n",
      "4  El representante de 'la U', Miguel Gómez, dijo...   \n",
      "\n",
      "                                           contenido  Etiqueta  \n",
      "0  Las autoridades están buscando al conductor de...  colombia  \n",
      "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
      "2  A las 7 de la noche me había quedado de encont...   archivo  \n",
      "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
      "4  “Estamos proponiendo la figura de un contrato ...   archivo  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta correcta al archivo\n",
    "ruta_archivo = r'C:\\Users\\analistaderiesgo1\\Procesamiento del Lenguaje Natural\\Datos\\Noticias.xlsx'\n",
    "\n",
    "# Cargar el archivo Excel en un DataFrame de pandas\n",
    "df = pd.read_excel(ruta_archivo)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar que se cargó correctamente\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Columna1                                            Enlaces  \\\n",
      "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
      "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
      "\n",
      "                                              Título  \\\n",
      "0  Operador de grúa quedó inconsciente tras agres...   \n",
      "1   Usaquén, primera en infracciones por mal parqueo   \n",
      "2  'Me atracaron y vi un arma que me heló la sang...   \n",
      "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
      "4  Radicado primer proyecto que autorizaría union...   \n",
      "\n",
      "                                                info  \\\n",
      "0  El conductor de una moto le lanzó el casco y p...   \n",
      "1  La localidad ocupa el primer lugar en comparen...   \n",
      "2  Un ciudadano relata cómo cuatro hombres lo rob...   \n",
      "3  Las zonas de restaurantes se convierten en par...   \n",
      "4  El representante de 'la U', Miguel Gómez, dijo...   \n",
      "\n",
      "                                           contenido  Etiqueta  \n",
      "0  Las autoridades están buscando al conductor de...  colombia  \n",
      "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
      "2  A las 7 de la noche me había quedado de encont...   archivo  \n",
      "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
      "4  “Estamos proponiendo la figura de un contrato ...   archivo  \n",
      "[[  16  301  137 2009   24 1923    1   18 3133    5   50 2421    4 4652\n",
      "     7  289 1091 4053    1    1    2  328  142 1342   14   18  429    4\n",
      "  1245  213    2 5074    3   17 1095  713 2038    4  207    3    4    5\n",
      "   243    4 5963    1    2  328   29    4   10  995   40    1    2 1275\n",
      "  1587   11 4169 3927    3    4  697 9785  859   11   11    9 3264    5\n",
      "   193    3   38  800  370    4 1923    1    2 3133   20 4652  145    4\n",
      "  4053    4  245 2436    4  777    3   20 1215   14   25    5  922  156\n",
      "     6    3]\n",
      " [   9 6284   83   15    9 5075 7729 5716   18 3086    5   52  598    5\n",
      "  4720  114    4 8783   89    5   10 3709    4 3686 2874    1    2  364\n",
      "  4516   11  350  296    3   28 3319   10  438 9786    5 1674    7    2\n",
      "  1718    8  410  195    4  260  876  180   11   18 6484    1 8924   78\n",
      "  4679   14  600 2437   14  138  290    4  280  193    5 3529   85    2\n",
      "   502   51 6219    2  364    3    4  396  131   16 9786    4  433  568\n",
      "  1034 1305    1    2 1718  410 2339    5   24   85    4    9 1195    1\n",
      "    16 9786]\n",
      " [   7   16  175    1    2  445  128  173 3543    1 1098   11 9618  293\n",
      "   493  176    6  174   21  519    2 3207   27  456 1067    1   20  265\n",
      "     6  128 1898   35 1525   14 5283  138    5    3   27  456 8021    3\n",
      "     2 3544    1    2 9476   11 3160    2 4680    1 1033    7 9326    1\n",
      "   201  265 2350    1 3303   29    3  412  445    1   28 3232 3687    3\n",
      "     4  316  281 3565   11   38 2787 2010    4 6220 7923    3    4  750\n",
      "     7   16   29   50    7    2  350 3160 3320    7  278   93   42   21\n",
      "   128 5076]\n",
      " [ 138   19   25    5   10  164  126   14    2  783    7    9 3206    6\n",
      "     5   10 4576    7 8327  682    3    6 1063    1    4 3007    4 3824\n",
      "     6   16  957    6 6284  828    8  316    1    2 4030    2  364   11\n",
      "  2162    2 1275 4055   63 1958 7375    6 5320    6    2  364 3566   63\n",
      "  1958   80    6  134   83    9   14  215  200   38 2457 5492    7    9\n",
      "  2232    8  280    3 1301   28  475  164    2 5321    7    9 1486   96\n",
      "   194    3    5   21  959  724    1    2  265  115    9 2851  137  226\n",
      "    24 9197]\n",
      " [3843    2 1125    1   17  967 1212    1  950 1699    5   21  135    7\n",
      "  2799 6968  234    7 2799   19   18 1125 4201    7    2    8 2548    6\n",
      "     5   50  507    7    9 6968    2  677    1  208  148    9  451    5\n",
      "  4653    2  345   14  346    9    6    9    1  269  322    6  162  722\n",
      "     5    3  506  439 2831    7   25    5   52 7043    2  402  415    4\n",
      "  2759    4  154  523    4  425  981   24  463    7   85    9 9788    1\n",
      "  2799    8  181 2644    6 7547   95   65   15  580  657    1 1347 2517\n",
      "     1  927]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Ruta correcta al archivo\n",
    "ruta_archivo = r'C:\\Users\\analistaderiesgo1\\Procesamiento del Lenguaje Natural\\Datos\\Noticias.xlsx'\n",
    "\n",
    "# Cargar el archivo Excel en un DataFrame de pandas\n",
    "df = pd.read_excel(ruta_archivo)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar que se cargó correctamente\n",
    "print(df.head())\n",
    "\n",
    "# Verifica si hay una columna con texto (ajusta el nombre de la columna si es necesario)\n",
    "# Supongamos que la columna con los textos se llama 'contenido'\n",
    "if 'contenido' not in df.columns:\n",
    "    print(\"La columna 'contenido' no existe en el archivo.\")\n",
    "else:\n",
    "    # 1. Asegurarse de que todos los valores en 'contenido' son cadenas de texto y reemplazar NaN\n",
    "    df['contenido'] = df['contenido'].fillna('').astype(str)\n",
    "    \n",
    "    # 2. Tokenización: convertir texto en secuencias de números\n",
    "    tokenizer = Tokenizer(num_words=10000)  # Limitar el vocabulario a las 10,000 palabras más comunes\n",
    "    tokenizer.fit_on_texts(df['contenido'])  # Ajustar el tokenizador a los textos\n",
    "    \n",
    "    # Convertir los textos en secuencias de números\n",
    "    secuencias = tokenizer.texts_to_sequences(df['contenido'])\n",
    "    \n",
    "    # 3. Padding: asegurar que todas las secuencias tengan la misma longitud\n",
    "    max_len = 100  # Longitud máxima de las secuencias, ajusta según tu necesidad\n",
    "    secuencias_pad = pad_sequences(secuencias, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Mostrar las primeras secuencias para verificar el preprocesamiento\n",
    "    print(secuencias_pad[:5])  # Muestra las primeras 5 secuencias con padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 2: Selección del Modelo BERT en Español\n",
    "\n",
    "Objetivo: Identificar y seleccionar un modelo BERT preentrenado en español adecuado para la tarea de clasificación de noticias.\n",
    "\n",
    "Acciones:\n",
    "\n",
    "1. Revisar los modelos disponibles en Hugging Face transformers que están preentrenados en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5685137510299683}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT preentrenado para clasificación\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "# Usar un pipeline para clasificación\n",
    "nlp = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Ejemplo de texto de noticia\n",
    "texto_noticia = \"El gobierno español ha anunciado nuevas medidas económicas.\"\n",
    "\n",
    "# Clasificación de la noticia\n",
    "resultado = nlp(texto_noticia)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Revisar los modelos disponibles en Hugging Face transformers que están preentrenados en español. \n",
    "3. Seleccionar el modelo más adecuado basándose en la tarea específica y la disponibilidad de recursos computacionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación y Recomendación:\n",
    "Eficiencia y Velocidad:\n",
    "mrm8488/bert-base-spanish-wwm-uncased probablemente sea el más rápido, ya que no distingue entre mayúsculas y minúsculas, lo que reduce el tamaño del vocabulario y los requisitos computacionales. Sin embargo, la falta de distinción de mayúsculas/minúsculas puede ser una limitación en tareas que dependan de esa información.\n",
    "bertin-project/bertin-roberta-base-spanish podría ser ligeramente más rápido que los modelos BERT debido a su preentrenamiento más eficiente, aunque puede ser algo más lento que el modelo uncased debido a su tamaño y enfoque de entrenamiento.\n",
    "Precisión y Efectividad:\n",
    "dccuchile/bert-base-spanish-wwm-cased tiene la ventaja de usar whole word masking, lo que puede mejorar la precisión en tareas que requieren una comprensión detallada del contexto de las palabras. Sin embargo, es más pesado computacionalmente debido al uso de la distinción de mayúsculas y minúsculas y el tamaño del modelo.\n",
    "bertin-project/bertin-roberta-base-spanish es un excelente modelo general, pero RoBERTa no utiliza whole word masking, lo que lo hace menos preciso que BERT para algunas tareas. Sin embargo, debido a su optimización, ofrece un buen equilibrio entre eficiencia y rendimiento, y es generalmente más rápido que el modelo BERT estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 3: Implementación del Modelo BERT en Español\n",
    "\n",
    "Objetivo: Construir y entrenar un modelo BERT en español para clasificar las noticias.\n",
    "\n",
    "Acciones:\n",
    "\n",
    "1. Utilizar la biblioteca transformers de Hugging Face para cargar el modelo BERT preentrenado seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Columna1', 'Enlaces', 'Título', 'info', 'contenido', 'Etiqueta'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a103336f0e94176bb1c6bcc2a1a5703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\analistaderiesgo1\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570083531ad64e99b5cbc1ab7e68c62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3ca0082c0949ef86834ae20410e290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Lista de textos de entrenamiento y validación (puedes usar tus propios textos)\n",
    "train_texts = [\"Texto de ejemplo 1\", \"Texto de ejemplo 2\", \"Texto de ejemplo 3\"]\n",
    "val_texts = [\"Texto de validación 1\", \"Texto de validación 2\"]\n",
    "\n",
    "# Tokenizar los textos de entrenamiento y validación\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Ahora `train_encodings` y `val_encodings` tienen los 'input_ids' y 'attention_mask' codificados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas originales: 10299    colombia\n",
      "3887      archivo\n",
      "12365    justicia\n",
      "4961      archivo\n",
      "5017      archivo\n",
      "Name: Etiqueta, dtype: object\n",
      "Etiquetas codificadas: [ 4  0 14  0  0  0  0  0  0  6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Etiquetas originales:\", train_labels.head())  # Etiquetas originales\n",
    "print(\"Etiquetas codificadas:\", train_labels_encoded[:10])  # Primeras 10 etiquetas codificadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Etiquetas de entrenamiento y validación (debe ser una lista con las etiquetas correspondientes)\n",
    "train_labels = [\"label1\", \"label2\", \"label1\", \"label3\", \"label2\"]\n",
    "val_labels = [\"label3\", \"label1\", \"label2\"]\n",
    "\n",
    "# Inicializar el codificador de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Codificar las etiquetas\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_encodings['input_ids']), len(train_encodings['attention_mask']), len(train_labels_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 3\n",
      "Attention Mask length: 3\n",
      "Labels length: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs length: {len(train_encodings['input_ids'])}\")\n",
    "print(f\"Attention Mask length: {len(train_encodings['attention_mask'])}\")\n",
    "print(f\"Labels length: {len(train_labels_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))  # Debería ser igual a len(train_labels)\n",
    "print(len(train_labels))  # Debe ser igual a len(train_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de train_texts: 3\n",
      "Longitud de train_labels: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Longitud de train_texts: {len(train_texts)}\")\n",
    "print(f\"Longitud de train_labels: {len(train_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos: ['Texto de ejemplo 1', 'Texto de ejemplo 2', 'Texto de ejemplo 3']\n",
      "Etiquetas: ['label1', 'label2', 'label1', 'label3', 'label2']\n"
     ]
    }
   ],
   "source": [
    "print(\"Textos:\", train_texts)\n",
    "print(\"Etiquetas:\", train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar las etiquetas para que coincidan con la cantidad de textos\n",
    "min_length = len(train_texts)\n",
    "train_labels = train_labels[:min_length]  # Recortar etiquetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label1', 'label2', 'label1']\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:5])  # Imprimir las primeras 5 etiquetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inicializar el codificador de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Codificar las etiquetas (esto convertirá las cadenas a números)\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# Ahora puedes crear el dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels_encoded)  # Etiquetas codificadas numéricamente\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_encoded[:5])  # Verificar las primeras 5 etiquetas codificadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de input_ids: 3\n",
      "Longitud de attention_mask: 3\n",
      "Longitud de train_labels_encoded: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Longitud de input_ids: {len(train_encodings['input_ids'])}\")\n",
    "print(f\"Longitud de attention_mask: {len(train_encodings['attention_mask'])}\")\n",
    "print(f\"Longitud de train_labels_encoded: {len(train_labels_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el encoder en todo el conjunto de datos\n",
    "all_labels = train_labels + val_labels  # Combinar etiquetas de entrenamiento y validación\n",
    "label_encoder.fit(all_labels)  # Ajustar el encoder\n",
    "\n",
    "# Ahora transforma ambas etiquetas\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas de validación antes de codificar: {'label2', 'label1', 'label3'}\n",
      "Etiquetas de entrenamiento: {'label2', 'label1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Etiquetas de validación antes de codificar:\", set(val_labels))\n",
    "print(\"Etiquetas de entrenamiento:\", set(train_labels))\n",
    "\n",
    "# Luego puedes intentar codificar las etiquetas de validación\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina las etiquetas de entrenamiento y validación\n",
    "all_labels = list(train_labels) + list(val_labels)\n",
    "\n",
    "# Ajustar el encoder en todo el conjunto de etiquetas\n",
    "label_encoder.fit(all_labels)  # Esto ajustará el codificador a todas las etiquetas\n",
    "\n",
    "# Codificar las etiquetas de entrenamiento y validación\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar etiquetas desconocidas de validación (etiquetas que no están en las de entrenamiento)\n",
    "valid_labels_cleaned = [label for label in val_labels if label in train_labels]\n",
    "\n",
    "# Ahora ajusta el codificador solo a las etiquetas de entrenamiento\n",
    "label_encoder.fit(train_labels)\n",
    "\n",
    "# Codificar etiquetas de entrenamiento y validación\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(valid_labels_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una clase adicional para 'label3'\n",
    "label_encoder.fit(list(train_labels) + ['label3'])  # Incluye 'label3' en el ajuste\n",
    "\n",
    "# Codificar etiquetas de entrenamiento y validación\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de train_encodings['input_ids']: 3\n",
      "Longitud de train_encodings['attention_mask']: 3\n",
      "Longitud de train_labels_encoded: 3\n",
      "Longitud de val_encodings['input_ids']: 2\n",
      "Longitud de val_encodings['attention_mask']: 2\n",
      "Longitud de val_labels_encoded: 3\n"
     ]
    }
   ],
   "source": [
    "# Verificar las longitudes de los datos\n",
    "print(f\"Longitud de train_encodings['input_ids']: {len(train_encodings['input_ids'])}\")\n",
    "print(f\"Longitud de train_encodings['attention_mask']: {len(train_encodings['attention_mask'])}\")\n",
    "print(f\"Longitud de train_labels_encoded: {len(train_labels_encoded)}\")\n",
    "\n",
    "print(f\"Longitud de val_encodings['input_ids']: {len(val_encodings['input_ids'])}\")\n",
    "print(f\"Longitud de val_encodings['attention_mask']: {len(val_encodings['attention_mask'])}\")\n",
    "print(f\"Longitud de val_labels_encoded: {len(val_labels_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos de validación: ['Texto de validación 1', 'Texto de validación 2']\n",
      "Etiquetas de validación: ['label3', 'label1', 'label2']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Textos de validación: {val_texts}\")\n",
    "print(f\"Etiquetas de validación: {val_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_encoded = val_labels_encoded[:len(val_encodings['input_ids'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Crear el dataset de entrenamiento\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels_encoded)  # Etiquetas codificadas numéricamente\n",
    ")\n",
    "\n",
    "# Crear el dataset de validación\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels_encoded)  # Etiquetas codificadas numéricamente para validación\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el dataloader para entrenamiento\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Crear el dataloader para validación\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Cargar el modelo preentrenado y ajustarlo para clasificación\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Ajusta el número de etiquetas\n",
    "\n",
    "# Definir el optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el dataset de entrenamiento\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels_encoded).long()  # Etiquetas codificadas numéricamente (tipo Long)\n",
    ")\n",
    "\n",
    "# Crear el dataset de validación\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels_encoded).long()  # Etiquetas codificadas numéricamente (tipo Long)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.long()  # Convertir las etiquetas a tipo Long en cada batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Crear dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Cargar el modelo preentrenado (ajustado a clasificación de secuencias)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # num_labels debe coincidir con el número de clases en tu problema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # La tasa de aprendizaje puede variar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Pérdida de entrenamiento: 1.049190640449524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Pérdida de validación: 1.1163835525512695, Precisión: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Pérdida de entrenamiento: 0.9632994532585144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Pérdida de validación: 1.139251708984375, Precisión: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Pérdida de entrenamiento: 1.008399248123169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Pérdida de validación: 1.1545491218566895, Precisión: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # para mostrar el progreso\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Mover el modelo a GPU si está disponible\n",
    "\n",
    "epochs = 3  # Número de épocas\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Establecer el modelo en modo de entrenamiento\n",
    "    total_loss = 0  # Pérdida acumulada para esta época\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Obtener los datos y moverlos a la GPU\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Limpiar los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Propagación hacia adelante\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # Calcular la pérdida y realizar retropropagación\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()  # Acumular la pérdida de la época\n",
    "\n",
    "        loss.backward()  # Retropropagación\n",
    "        optimizer.step()  # Actualización de los pesos\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Pérdida de entrenamiento: {avg_train_loss}\")\n",
    "    \n",
    "    # Evaluación en el conjunto de validación\n",
    "    model.eval()  # Establecer el modelo en modo de evaluación\n",
    "    total_eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        # Obtener los datos y moverlos a la GPU\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Desactivar el cálculo de gradientes\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Calcular las predicciones\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "    accuracy = correct_predictions / len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Pérdida de validación: {avg_eval_loss}, Precisión: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "model.save_pretrained(\"path_to_save_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Monitoreo de la Métrica de Evaluación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en validación: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Para obtener métricas adicionales\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch in tqdm(val_dataloader):\n",
    "    input_ids = batch[0].to(device)\n",
    "    attention_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    y_true.extend(labels.cpu().numpy())\n",
    "    y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Obtener precisión y reporte de clasificación\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Precisión en validación: {accuracy}\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Uso de Scheduler para Ajustar la Tasa de Aprendizaje\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Número total de pasos de entrenamiento\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Scheduler para la tasa de aprendizaje\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "scheduler.step()  # Actualizar el scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Guardar y Cargar el Mejor Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Pérdida de entrenamiento: 0.8590553402900696, Pérdida de validación: 1.1762371063232422, Precisión: 0.5\n",
      "Modelo guardado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Pérdida de entrenamiento: 0.8703435063362122, Pérdida de validación: 1.2099491357803345, Precisión: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Pérdida de entrenamiento: 0.8129927515983582, Pérdida de validación: 1.1576114892959595, Precisión: 0.5\n",
      "Modelo guardado!\n"
     ]
    }
   ],
   "source": [
    "# Inicializar una variable para almacenar la mejor pérdida de validación\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "    accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Pérdida de entrenamiento: {avg_train_loss}, \"\n",
    "          f\"Pérdida de validación: {avg_eval_loss}, Precisión: {accuracy}\")\n",
    "\n",
    "    # Guardar el modelo si la pérdida de validación es la mejor\n",
    "    if avg_eval_loss < best_val_loss:\n",
    "        best_val_loss = avg_eval_loss\n",
    "        model.save_pretrained(\"path_to_save_best_model\")\n",
    "        print(\"Modelo guardado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los textos y las etiquetas para el conjunto de prueba (si las tienes)\n",
    "test_texts = [\"Texto de prueba 1\", \"Texto de prueba 2\"]  # ejemplo de textos de prueba\n",
    "test_labels = [\"label1\", \"label2\"]  # ejemplo de etiquetas de prueba\n",
    "\n",
    "# Codificar los textos del conjunto de prueba\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Codificar las etiquetas (asegurarte de que tienes un mapeo adecuado)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Crear el dataset para el conjunto de prueba\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(test_labels_encoded).long()  # Etiquetas codificadas como long\n",
    ")\n",
    "\n",
    "# Ahora puedes crear el DataLoader para el conjunto de prueba\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Poner el modelo en modo evaluación\n",
    "y_true_test = []  # Lista para las etiquetas verdaderas\n",
    "y_pred_test = []  # Lista para las predicciones del modelo\n",
    "\n",
    "# No calcular gradientes durante la evaluación\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Hacer las predicciones\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Guardar las etiquetas verdaderas y las predicciones\n",
    "        y_true_test.extend(labels.cpu().numpy())\n",
    "        y_pred_test.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calcular las métricas (por ejemplo, precisión, F1, etc.)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class_weights = torch.tensor([1.0, 2.0])  # Ajustar según la distribución de clases\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101, 3793, 2080, 2139, 1041, 6460, 8737, 4135, 1015,  102],\n",
      "        [ 101, 3793, 2080, 2139, 1041, 6460, 8737, 4135, 1017,  102],\n",
      "        [ 101, 3793, 2080, 2139, 1041, 6460, 8737, 4135, 1016,  102]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([0, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)  # Imprime el contenido del batch para inspeccionar su formato\n",
    "    break  # Solo para verificar el primer batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([3, 10])\n",
      "labels.shape: torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"inputs.shape: {inputs.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si inputs tiene forma (3, seq_len) y labels tiene forma (30,), deberás corregir la carga de los datos\n",
    "assert inputs.shape[0] == labels.shape[0], f\"Batch size mismatch: {inputs.shape[0]} vs {labels.shape[0]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 3, Labels batch size: 3\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    inputs = batch[0].to(device)  # Entrada (input_ids, atención, etc.)\n",
    "    labels = batch[1].to(device)  # Etiquetas (labels)\n",
    "\n",
    "    print(f\"Batch size: {inputs.shape[0]}, Labels batch size: {labels.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\analistaderiesgo1\\AppData\\Local\\Temp\\ipykernel_5448\\991071281.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)  # Asegúrate de que sean enteros largos\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor(labels, dtype=torch.long)  # Asegúrate de que sean enteros largos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar un dataset de Hugging Face (puedes reemplazar esto con el dataset adecuado)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Ver cómo se organiza el dataset\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Suponiendo que tu dataset tiene un formato correcto de entradas y etiquetas\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que num_labels esté correctamente definido en la configuración del modelo\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Aquí asumes que cada 'batch' es una lista de diccionarios con 'input_ids' y 'labels'\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Convierte a tensores\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return {'input_ids': input_ids, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Accede al split de entrenamiento\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Crea el DataLoader a partir del conjunto de datos de entrenamiento\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Accede al primer ejemplo del conjunto de entrenamiento\n",
    "example = dataset['train'][0]\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
      "          2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
      "          2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
      "          2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
      "          1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
      "          2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
      "          6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
      "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
      "          5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
      "         14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
      "          1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
      "          2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
      "         25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
      "          5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
      "          1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
      "          8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
      "          2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
      "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
      "          8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
      "          2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,  2428,\n",
      "          1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,  2261,  1998,\n",
      "          2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,  2025,  2915,\n",
      "          2066,  2070, 10036,  2135,  2081, 22555,  2080,  1012,  2096,  2026,\n",
      "          2406,  3549,  2568,  2424,  2009, 16880,  1010,  1999,  4507,  3348,\n",
      "          1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
      "          1012,  2130, 13749,  7849, 24544,  1010, 15835,  2037,  3437,  2000,\n",
      "          2204,  2214,  2879,  2198,  4811,  1010,  2018,  3348,  5019,  1999,\n",
      "          2010,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
      "          1028,  1045,  2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,\n",
      "          2755,  2008,  2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,\n",
      "          2005,  6018,  5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,\n",
      "          2191,  2769,  2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,\n",
      "          1012,  1045,  2572,  8025,  1011,  3756,  2003,  1037,  2204,  2143,\n",
      "          2005,  3087,  5782,  2000,  2817,  1996,  6240,  1998, 14629,  1006,\n",
      "          2053, 26136,  3832,  1007,  1997,  4467,  5988,  1012,  2021,  2428,\n",
      "          1010,  2023,  2143,  2987,  1005,  1056,  2031,  2172,  1997,  1037,\n",
      "          5436,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizar el texto del ejemplo\n",
    "inputs = tokenizer(example['text'], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'], 'label': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convertir el conjunto de datos a formato de Dataset de Hugging Face\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': [example['text']],  # Aquí puedes agregar más ejemplos\n",
    "    'label': [example['label']]  # Asegúrate de que las etiquetas también estén presentes\n",
    "})\n",
    "\n",
    "# Usar un DataLoader para manejar los lotes de datos\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "# Iterar sobre el DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento de datos y creación de DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf38ad00ae54f309bdcca612cc25083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Cargar el tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Supongamos que tu dataset es un diccionario similar a lo que mencionaste:\n",
    "dataset = {\n",
    "    'train': [\n",
    "        {'text': 'I rented I AM CURIOUS-YELLOW from my video store...', 'label': 0},\n",
    "        {'text': 'This is an example of a movie review...', 'label': 1},\n",
    "        # ... Agrega más ejemplos aquí\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convertir el conjunto de datos a un formato compatible con Hugging Face\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': [item['text'] for item in dataset['train']],\n",
    "    'label': [item['label'] for item in dataset['train']]\n",
    "})\n",
    "\n",
    "# Preprocesamiento: Tokenización del texto\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Aplicar el preprocesamiento al dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Crear un DataLoader para manejar los lotes\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch.optim as optim\n",
    "\n",
    "# Cargar el modelo preentrenado con 2 clases de salida (ajusta según el número de clases)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Mover el modelo a GPU si está disponible\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir el optimizador y la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Definir la función de pérdida (para clasificación)\n",
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciclo de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['This is an example of a movie review...', 'I rented I AM CURIOUS-YELLOW from my video store...'], 'label': tensor([1, 0]), 'input_ids': [tensor([101, 101]), tensor([2023, 1045]), tensor([ 2003, 12524]), tensor([2019, 1045]), tensor([2742, 2572]), tensor([1997, 8025]), tensor([1037, 1011]), tensor([3185, 3756]), tensor([3319, 2013]), tensor([1012, 2026]), tensor([1012, 2678]), tensor([1012, 3573]), tensor([ 102, 1012]), tensor([   0, 1012]), tensor([   0, 1012]), tensor([  0, 102])], 'token_type_ids': [tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0])], 'attention_mask': [tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1])]}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)  # Imprime el contenido de un lote para inspeccionar su estructura\n",
    "    break  # Solo imprime el primer lote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [tensor([101, 101]), tensor([2023, 1045]), tensor([ 2003, 12524]), tensor([2019, 1045]), tensor([2742, 2572]), tensor([1997, 8025]), tensor([1037, 1011]), tensor([3185, 3756]), tensor([3319, 2013]), tensor([1012, 2026]), tensor([1012, 2678]), tensor([1012, 3573]), tensor([ 102, 1012]), tensor([   0, 1012]), tensor([   0, 1012]), tensor([  0, 102])]\n",
      "input_ids shape: torch.Size([16, 2])\n",
      "labels shape: torch.Size([2])\n",
      "Error: El tamaño de lote no coincide: 16 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\analistaderiesgo1\\AppData\\Local\\Temp\\ipykernel_5448\\448092226.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.stack([torch.tensor(item) for item in batch['input_ids']]).to(device)\n",
      "C:\\Users\\analistaderiesgo1\\AppData\\Local\\Temp\\ipykernel_5448\\448092226.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.stack([torch.tensor(item) for item in batch['attention_mask']]).to(device)\n",
      "C:\\Users\\analistaderiesgo1\\AppData\\Local\\Temp\\ipykernel_5448\\448092226.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label']).to(device)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    # Verifica el contenido de batch\n",
    "    print(f\"input_ids: {batch['input_ids']}\")\n",
    "    \n",
    "    # Convertir las listas de input_ids y labels a tensores correctamente\n",
    "    input_ids = torch.stack([torch.tensor(item) for item in batch['input_ids']]).to(device)\n",
    "    attention_mask = torch.stack([torch.tensor(item) for item in batch['attention_mask']]).to(device)\n",
    "    labels = torch.tensor(batch['label']).to(device)\n",
    "\n",
    "    # Verifica el tamaño de los lotes\n",
    "    print(f\"input_ids shape: {input_ids.shape}\")\n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Asegúrate de que las dimensiones coinciden\n",
    "    if input_ids.shape[0] != labels.shape[0]:\n",
    "        print(f\"Error: El tamaño de lote no coincide: {input_ids.shape[0]} != {labels.shape[0]}\")\n",
    "        continue  # Salta este lote si no coinciden los tamaños\n",
    "\n",
    "    # Continuar con la pasada del modelo\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Pasar los datos al modelo\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    # Obtener la pérdida\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Realizar la retropropagación\n",
    "    loss.backward()\n",
    "\n",
    "    # Actualizar los parámetros del modelo\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(f\"Labels: {batch['label']}\")  # Imprime las etiquetas para verificar su forma\n",
    "    break  # Solo imprime el primer lote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Asegúrate de que el DataLoader se esté configurando correctamente\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['This is an example of a movie review...', 'I rented I AM CURIOUS-YELLOW from my video store...'], 'label': tensor([1, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Iterar sobre los lotes de datos\n",
    "for batch in train_dataloader:\n",
    "    # Imprimir el contenido de 'batch' para ver qué claves contiene\n",
    "    print(batch)\n",
    "    break  # Imprimir solo el primer lote para inspección\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 16])\n",
      "attention_mask shape: torch.Size([2, 16])\n",
      "labels shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizer (usando BERT como ejemplo)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Iterar sobre los lotes de datos\n",
    "for batch in train_dataloader:\n",
    "    # Tokenizar los textos\n",
    "    encodings = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "    # Obtener input_ids y attention_mask\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # Obtener las etiquetas\n",
    "    labels = batch['label'].to(device)\n",
    "\n",
    "    # Verificar las formas de input_ids y labels\n",
    "    print(f\"input_ids shape: {input_ids.shape}\")\n",
    "    print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "    # Asegurarse de que las dimensiones coinciden\n",
    "    if input_ids.shape[0] != labels.shape[0]:\n",
    "        print(f\"Error: El tamaño de lote no coincide: {input_ids.shape[0]} != {labels.shape[0]}\")\n",
    "        continue  # Salta este lote si no coinciden los tamaños\n",
    "\n",
    "    # Optimizar el modelo\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Pasar los datos al modelo\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    # Obtener la pérdida\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Realizar la retropropagación\n",
    "    loss.backward()\n",
    "\n",
    "    # Actualizar los parámetros del modelo\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 16])\n",
      "attention_mask shape: torch.Size([2, 16])\n",
      "labels shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Inicializar el tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ejemplo de datos\n",
    "texts = [\"This is an example of a movie review...\", \"I rented I AM CURIOUS-YELLOW from my video store...\"]\n",
    "labels = [1, 0]\n",
    "\n",
    "# Tokenizar los textos\n",
    "encoding = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Crear el dataset con los input_ids y attention_mask\n",
    "dataset = TensorDataset(encoding['input_ids'], encoding['attention_mask'], torch.tensor(labels))\n",
    "\n",
    "# Crear el dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Iterar sobre los lotes del dataloader\n",
    "for batch in train_dataloader:\n",
    "    # Acceder a los datos tokenizados correctamente\n",
    "    input_ids = batch[0].to(device)\n",
    "    attention_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "    \n",
    "    print(f\"input_ids shape: {input_ids.shape}\")\n",
    "    print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Aquí puedes realizar el entrenamiento o lo que necesites hacer\n",
    "    break  # Para detenerse después del primer lote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Loss: 0.7392\n",
      "Accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "Loss: 0.6839\n",
      "Accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "Loss: 0.5493\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definir el dispositivo (GPU si está disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar el modelo preentrenado para clasificación\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Mover el modelo al dispositivo\n",
    "model.to(device)\n",
    "\n",
    "# Definir el optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Definir el DataLoader para los lotes de datos\n",
    "train_dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Configurar el ciclo de entrenamiento\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Poner el modelo en modo entrenamiento\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        # Mover los datos al dispositivo\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        # Limpiar los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pasar los datos al modelo\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Obtener la pérdida\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Hacer retropropagación\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Predicciones\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Calcular las predicciones correctas\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "    # Imprimir los resultados por época\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular Métricas de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Establecer el modelo en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Variables para almacenar las predicciones y etiquetas reales\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Realizar inferencias sobre el conjunto de prueba/validación\n",
    "for batch in train_dataloader:  # Si tienes un dataloader de validación, usa ese\n",
    "    input_ids = batch[0].to(device)\n",
    "    attention_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "\n",
    "    # Obtener las predicciones del modelo\n",
    "    with torch.no_grad():  # No se calcula gradiente\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Obtener las predicciones finales\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Almacenar las predicciones y las etiquetas\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calcular las métricas\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar las Curvas de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2023,  2003,  2019,  2742,  1997,  1037,  3185,  3319,  1012,\n",
      "          1012,  1012,   102,     0,     0,     0],\n",
      "        [  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
      "          2678,  3573,  1012,  1012,  1012,   102]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([1, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    # Acceder a los tensores directamente (sin claves adicionales)\n",
    "    input_ids = batch[0].to(device)  # Tensor de input_ids\n",
    "    attention_mask = batch[1].to(device)  # Tensor de attention_mask\n",
    "    labels = batch[2].to(device)  # Tensor de labels\n",
    "\n",
    "    # Continuar con tu código de entrenamiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 0.7529 - Accuracy: 0.5000\n",
      "Epoch 2/3 - Loss: 0.7081 - Accuracy: 0.5000\n",
      "Epoch 3/3 - Loss: 0.6958 - Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configurar el optimizador y la función de pérdida\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Número de épocas\n",
    "epochs = 3\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Configuramos el modelo en modo de entrenamiento\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Inicializar los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Hacer la predicción\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calcular el gradiente y actualizar los parámetros\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Acumular métricas\n",
    "        running_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    # Promediar la pérdida y calcular precisión\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supongamos que tienes un Dataset con 'text' y 'labels'\n",
    "texts = [\"texto1\", \"texto2\", \"texto3\", ...]  # Lista de textos\n",
    "labels = [0, 1, 0, ...]  # Lista de etiquetas\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y validación\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)  # 80% entrenamiento, 20% validación\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar el conjunto de validación\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Crear un dataset para validación\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels)\n",
    ")\n",
    "\n",
    "# Crear el DataLoader de validación\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00       1.0\n",
      "     Class 1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluación en el conjunto de validación\n",
    "model.eval()  # Configuramos el modelo en modo de evaluación\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():  # No necesitamos calcular gradientes durante la evaluación\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Hacer la predicción\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Obtener las predicciones\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Calcular métricas\n",
    "report = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels: [0]\n",
      "Predicted labels: [1]\n"
     ]
    }
   ],
   "source": [
    "# Verificar las primeras predicciones\n",
    "print(\"True labels:\", y_true[:10])\n",
    "print(\"Predicted labels:\", y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of val_texts: 1\n",
      "Length of val_labels: 1\n",
      "Text: texto1\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of val_texts:\", len(val_texts))\n",
    "print(\"Length of val_labels:\", len(val_labels))\n",
    "\n",
    "# Asegúrate de que las listas tengan suficientes elementos\n",
    "for i in range(min(5, len(val_texts))):  # Solo iterar hasta el tamaño de las listas\n",
    "    print(f\"Text: {val_texts[i]}\")\n",
    "    print(f\"Label: {val_labels[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['texto1']\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(val_texts[:5])  # Verificar las primeras 5 entradas de val_texts\n",
    "print(val_labels[:5])  # Verificar las primeras 5 entradas de val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101, 3793, 2080, 2487,  102]]), tensor([[1, 1, 1, 1, 1]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "# Verificar el DataLoader\n",
    "for batch in validation_dataloader:\n",
    "    print(batch)  # Imprime el primer lote de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: tensor([1])\n",
      "Etiqueta real: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Suponiendo que has cargado el modelo y el tokenizador BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Paso 1: Tokenizar el texto\n",
    "text = val_texts[0]  # Este es el único texto\n",
    "encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "# Paso 2: Obtener input_ids y attention_mask\n",
    "input_ids = encoding['input_ids']  # Tensor de input_ids\n",
    "attention_mask = encoding['attention_mask']  # Tensor de attention_mask\n",
    "\n",
    "# Paso 3: Enviar a device (CPU o GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Paso 4: Obtener la predicción del modelo\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)  # Obtener la clase con la mayor probabilidad\n",
    "\n",
    "    print(f\"Predicción: {predictions}\")\n",
    "    print(f\"Etiqueta real: {val_labels[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.0000\n",
      "Exactitud: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Inicializar listas para las predicciones y etiquetas reales\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterar sobre todo el conjunto de validación\n",
    "for i in range(len(val_texts)):\n",
    "    # Tokenización del texto\n",
    "    encoding = tokenizer(val_texts[i], truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    \n",
    "    # Transferir los datos al dispositivo\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Obtener la predicción del modelo\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Poner el modelo en modo evaluación\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()  # Obtener la clase predicha\n",
    "    \n",
    "    # Almacenar las predicciones y las etiquetas reales\n",
    "    y_pred.append(prediction)\n",
    "    y_true.append(val_labels[i])\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='binary', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred, average='binary', zero_division=1)\n",
    "f1 = f1_score(y_true, y_pred, average='binary', zero_division=1)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Exactitud: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(val_labels))  # Reemplaza val_labels por las etiquetas de tu conjunto de validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(val_texts[i], padding=True, truncation=True, max_length=512, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  # Aumenta el número de épocas para entrenar más tiempo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asegúrate de no congelar las capas si estás usando BERT preentrenado.\n",
    "model.train()  # Asegúrate de que el modelo esté en modo entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mostrar el reporte completo de clasificación\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1, Ellipsis: 1, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(train_labels))  # Asegúrate de que haya un balance entre las clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       1.0\n",
      "           1       0.00      1.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.50      0.50      0.00       1.0\n",
      "weighted avg       1.00      0.00      0.00       1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Asegúrate de que el reporte ignore las clases sin predicciones\n",
    "print(classification_report(y_true, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n"
     ]
    }
   ],
   "source": [
    "print(set(val_labels))  # Verifica que las etiquetas sean 0 o 1, y que no haya valores extraños\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(val_labels))  # Verifica la distribución de las clases en el conjunto de validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class_weights = torch.tensor([1.0, 10.0]).to(device)  # Ajusta el peso según la desbalance de clases\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       1.0\n",
      "           1       0.00      1.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.50      0.50      0.00       1.0\n",
      "weighted avg       1.00      0.00      0.00       1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular los pesos de las clases inversamente proporcionales a su frecuencia\n",
    "class_counts = torch.tensor([1, 1]).float()  # Ajusta esto con las frecuencias de tus clases\n",
    "class_weights = 1. / class_counts\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # Puedes cambiar este número según lo que necesites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    # Si `batch` es una lista, asignar los tensores directamente\n",
    "    input_ids, attention_mask, labels = batch\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Limpiar gradientes\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Pasar por el modelo\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # Asegúrate de que esto es correcto dependiendo de tu modelo\n",
    "    \n",
    "    # Calcular la pérdida usando CrossEntropyLoss con los pesos\n",
    "    loss = criterion(logits, labels)\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    # Realizar retropropagación y optimización\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calcular precisión (acierto)\n",
    "    _, preds = torch.max(logits, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total_correct += correct\n",
    "    total_samples += labels.size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2023,  2003,  2019,  2742,  1997,  1037,  3185,  3319,  1012,\n",
      "          1012,  1012,   102,     0,     0,     0],\n",
      "        [  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
      "          2678,  3573,  1012,  1012,  1012,   102]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([1, 0])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)  # Para inspeccionar cómo está organizado el batch\n",
    "    break  # Solo inspeccionamos el primer batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7150, Accuracy: 0.5000\n",
      "Epoch 2/5, Loss: 0.6329, Accuracy: 0.5000\n",
      "Epoch 3/5, Loss: 0.4782, Accuracy: 1.0000\n",
      "Epoch 4/5, Loss: 0.5468, Accuracy: 1.0000\n",
      "Epoch 5/5, Loss: 0.4789, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Cambiar el modelo al modo de entrenamiento\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        # Desempaquetar los tensores directamente desde el batch\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        # Mover los tensores al dispositivo (GPU o CPU)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Limpiar gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pasar por el modelo\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Asegúrate de que esto es correcto dependiendo de tu modelo\n",
    "        \n",
    "        # Calcular la pérdida usando CrossEntropyLoss con los pesos\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Realizar retropropagación y optimización\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular precisión (acierto)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    # Calcular métricas al final de la época\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "    epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\analistaderiesgo1\\AppData\\Local\\miniconda3\\envs\\proyecto-1-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de validación\n",
    "model.eval()  # Cambiar el modelo al modo de evaluación (sin gradientes)\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():  # No necesitamos calcular gradientes durante la evaluación\n",
    "    for batch in validation_dataloader:  # Asumiendo que tienes un validation_dataloader\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        # Mover los tensores al dispositivo\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Obtener las predicciones\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Obtener las predicciones\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        \n",
    "        # Contar los correctos\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Almacenar las predicciones y etiquetas para calcular métricas adicionales\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calcular la precisión en el conjunto de validación\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calcular otras métricas de clasificación, como F1-score, precisión y recall\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Definir la arquitectura de tu modelo (ejemplo simple)\n",
    "        self.fc = nn.Linear(768, 2)  # Ejemplo: capas totalmente conectadas\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\analistaderiesgo1\\AppData\\Local\\Temp\\ipykernel_5448\\136075506.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"), strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=['classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pth\"), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Verifica los parámetros del modelo\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto-1-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
